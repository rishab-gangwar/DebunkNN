# Advantage of Having Batch Normalization Layer in CNN
>This part explores the effect of bn Layer on training CNN.
>>* We have taken two different cnn architectures ,with difference of bn layer.<br>

|**`CNN without BatchNorm`**|**`CNN with BatchNorm`**|
|:---------------------------:|:------------------:|
|![GitHub Logo](https://github.com/rishab-gangwar/nn_from_scratch/blob/master/BatchNormCNN/MyCNNnet.png)|![oogh](https://github.com/rishab-gangwar/nn_from_scratch/blob/master/BatchNormCNN/MyCNNBN.png)|
|  Input Data distribution  |![GitHub Logo](https://github.com/rishab-gangwar/nn_from_scratch/blob/master/BatchNormCNN/INITdist.png) |
|Loss plot after 1 epoch  |![oo](https://github.com/rishab-gangwar/nn_from_scratch/blob/master/BatchNormCNN/loss1.png)|
|data distribution after 10th epoch |![oo](https://github.com/rishab-gangwar/nn_from_scratch/blob/master/BatchNormCNN/DistAfter10.png)|
|Loss plot after 24 epoch |![oo](https://github.com/rishab-gangwar/nn_from_scratch/blob/master/BatchNormCNN/loss24.png)|
|Loss plot after 40 epoch  |![oo](https://github.com/rishab-gangwar/nn_from_scratch/blob/master/BatchNormCNN/loss40.png)|
|data distribution after 10th epoch |![oo](https://github.com/rishab-gangwar/nn_from_scratch/blob/master/BatchNormCNN/DistAfter10.png)|
|output distribution  |![oo](https://github.com/rishab-gangwar/nn_from_scratch/blob/master/BatchNormCNN/outputdist.png)|
