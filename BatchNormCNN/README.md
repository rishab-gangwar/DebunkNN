# Advantage of Having Batch Normalization Layer in CNN
>This part of repo explores the effect of batch normalization Layer on training Conv. Neural Nets.
>>* We have taken two different cNN with only diference in the presence of Batch Normalization in one.<br>

|**`CNN without BatchNorm`**|**`CNN with BatchNorm`**|
|:---------------------------:|:------------------:|
|![GitHub Logo](https://github.com/rishab-gangwar/nn_from_scratch/blob/master/BatchNormCNN/MyCNNnet.png)|![oogh](https://github.com/rishab-gangwar/nn_from_scratch/blob/master/BatchNormCNN/MyCNNBN.png)|
|  Input Data distribution  |![GitHub Logo](https://github.com/rishab-gangwar/nn_from_scratch/blob/master/BatchNormCNN/INITdist.png) |
|Loss plot after 1 epoch  |![oo](https://github.com/rishab-gangwar/nn_from_scratch/blob/master/BatchNormCNN/loss1.png)|
|data distribution after 10th epoch |![oo](https://github.com/rishab-gangwar/nn_from_scratch/blob/master/BatchNormCNN/DistAfter10.png)|
|Loss plot after 24 epoch |![oo](https://github.com/rishab-gangwar/nn_from_scratch/blob/master/BatchNormCNN/loss24.png)|
|Loss plot after 40 epoch  |![oo](https://github.com/rishab-gangwar/nn_from_scratch/blob/master/BatchNormCNN/loss40.png)|
|data distribution after 10th epoch |![oo](https://github.com/rishab-gangwar/nn_from_scratch/blob/master/BatchNormCNN/outputdist.png)|
